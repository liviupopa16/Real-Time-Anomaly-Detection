{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE FACE DETECTION (MEDIA PIPE + FER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('FER_64.5acc_0.99loss.h5')\n",
    "# model = tf.keras.models.load_model('model.h5')\n",
    "labels_dict = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection(0.75)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = faceDetection.process(imgRGB)\n",
    "    # print(results)\n",
    "\n",
    "    if results.detections:\n",
    "        # for id, detection in enumerate(results.detections):       // for multiple faces\n",
    "        detection = results.detections[0]  # Get the first face detection result\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "        ih, iw, ic = img.shape\n",
    "        bbox = (int(bboxC.xmin * iw), int(bboxC.ymin * ih),\n",
    "                int(bboxC.width * iw), int(bboxC.height * ih))\n",
    "\n",
    "        face_img = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]  # Extract the face region\n",
    "        if face_img.shape[0] > 0 and face_img.shape[1] > 0:\n",
    "            # Resize the face image to match the input size of the trained model\n",
    "            face_img = cv2.resize(face_img, (48, 48))\n",
    "\n",
    "            # Convert the face image to grayscale\n",
    "            face_gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Normalize the pixel values to be in the range [0, 1]\n",
    "            face_gray = face_gray / 255.0\n",
    "\n",
    "            # Reshape the face image to match the input shape of the trained model\n",
    "            face_gray = face_gray.reshape(1, 48, 48, 1)\n",
    "        else:\n",
    "            print(\"Error: Failed to extract face region\")\n",
    "        # Predict the emotion using the trained model\n",
    "        emotion_probs = model.predict(face_gray)[0]\n",
    "        emotion_id = tf.argmax(emotion_probs)\n",
    "        emotion_label = labels_dict[emotion_id.numpy()]\n",
    "\n",
    "        cv2.putText(img, emotion_label, (bbox[0], bbox[1] - 50), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 255), 2)\n",
    "\n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), (255, 0, 255), 2)\n",
    "        # cv2.putText(img, f'{int(detection.score[0] * 100)}%', (bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIFACE DETECTION (HAARCASCADE + FER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at FER_64.5acc_0.99loss.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[0;32m      5\u001b[0m \u001b[39m# model=load_model('model_file_30epochs.h5')\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model\u001b[39m=\u001b[39mload_model(\u001b[39m'\u001b[39;49m\u001b[39mFER_64.5acc_0.99loss.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# model=load_model('model_mobnet.h5')\u001b[39;00m\n\u001b[0;32m      9\u001b[0m video\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\detectron_env\\lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda\\envs\\detectron_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\anaconda\\envs\\detectron_env\\lib\\site-packages\\keras\\saving\\legacy\\save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    235\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(\n\u001b[0;32m    236\u001b[0m             filepath_str, \u001b[39mcompile\u001b[39m, options\n\u001b[0;32m    237\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at FER_64.5acc_0.99loss.h5"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# model=load_model('model_file_30epochs.h5')\n",
    "model=load_model('FER_64.5acc_0.99loss.h5')\n",
    "# model=load_model('model_mobnet.h5')\n",
    "\n",
    "video=cv2.VideoCapture(0)\n",
    "\n",
    "frontalfaceDetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "profilefaceDetect=cv2.CascadeClassifier('haarcascade_profileface.xml')\n",
    "labels_dict={0:'Angry',1:'Disgust', 2:'Fear', 3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect frontal faces\n",
    "    frontal_faces = frontalfaceDetect.detectMultiScale(gray, 1.3, 5)\n",
    "    if len(frontal_faces) > 0:\n",
    "        for x, y, w, h in frontal_faces:\n",
    "            sub_face_img = gray[y:y+h, x:x+w]\n",
    "            resized = cv2.resize(sub_face_img, (48, 48))\n",
    "            normalize = resized / 255.0\n",
    "            reshaped = np.reshape(normalize, (1, 48, 48, 1))\n",
    "            result = model.predict(reshaped)\n",
    "            label = np.argmax(result, axis=1)[0]\n",
    "            print(label)\n",
    "            #cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            #cv2.rectangle(frame, (x, y-40), (x+w, y), (50, 50, 255), -1)\n",
    "            cv2.putText(frame, labels_dict[label], (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"frontal face\", (7, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    else:\n",
    "        # Detect profile faces\n",
    "        profile_faces = profilefaceDetect.detectMultiScale(gray, 1.3, 5)\n",
    "        if len(profile_faces) > 0:\n",
    "            for x, y, w, h in profile_faces:\n",
    "                sub_face_img = gray[y:y+h, x:x+w]\n",
    "                resized = cv2.resize(sub_face_img, (48, 48))\n",
    "                normalize = resized / 255.0\n",
    "                reshaped = np.reshape(normalize, (1, 48, 48, 1))\n",
    "                result = model.predict(reshaped)\n",
    "                label = np.argmax(result, axis=1)[0]\n",
    "                print(label)\n",
    "                #cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                #cv2.rectangle(frame, (x, y-40), (x+w, y), (50, 50, 255), -1)\n",
    "                cv2.putText(frame, labels_dict[label], (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"profile face\", (7, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                \n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
